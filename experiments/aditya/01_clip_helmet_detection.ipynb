{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from on_field_gear_compliance_hackathon.constants import DATA_DIR, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'openai/clip-vit-base-patch32'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "image_processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(\n",
    "    text=[\"a person wearing helmet\", \"a person not wearing helmet\"], images=image, return_tensors=\"pt\", padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  2533,  3309, 11122, 49407, 49407],\n",
       "        [49406,   320,  2533,   783,  3309, 11122, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[-9.3096e-01, -9.8935e-01, -1.0185e+00,  ...,  2.5152e-01,\n",
       "            3.5371e-01,  4.2670e-01],\n",
       "          [-9.4555e-01, -1.0185e+00, -1.0039e+00,  ...,  1.0553e-01,\n",
       "            1.4933e-01,  1.7853e-01],\n",
       "          [-1.0185e+00, -1.0039e+00, -8.8716e-01,  ...,  9.0935e-02,\n",
       "            9.0935e-02,  9.0935e-02],\n",
       "          ...,\n",
       "          [ 1.8573e+00,  1.8865e+00,  1.9011e+00,  ..., -4.2001e-01,\n",
       "           -3.0322e-01, -2.4483e-01],\n",
       "          [ 1.8427e+00,  1.8281e+00,  1.8427e+00,  ..., -5.0760e-01,\n",
       "           -5.0760e-01, -4.4921e-01],\n",
       "          [ 1.8135e+00,  1.7990e+00,  1.7844e+00,  ..., -5.8059e-01,\n",
       "           -6.2439e-01, -5.5140e-01]],\n",
       "\n",
       "         [[-1.7628e-01, -2.3631e-01, -2.6633e-01,  ...,  5.7411e-01,\n",
       "            6.3414e-01,  6.6415e-01],\n",
       "          [-1.9129e-01, -2.6633e-01, -2.8134e-01,  ...,  4.2403e-01,\n",
       "            4.3904e-01,  4.2403e-01],\n",
       "          [-2.5132e-01, -2.6633e-01, -1.9129e-01,  ...,  4.5404e-01,\n",
       "            4.2403e-01,  3.6400e-01],\n",
       "          ...,\n",
       "          [ 1.5646e+00,  1.5946e+00,  1.6247e+00,  ..., -8.6664e-01,\n",
       "           -7.6158e-01, -7.0155e-01],\n",
       "          [ 1.4896e+00,  1.5196e+00,  1.5646e+00,  ..., -9.4168e-01,\n",
       "           -9.2667e-01, -8.6664e-01],\n",
       "          [ 1.4446e+00,  1.4596e+00,  1.4896e+00,  ..., -1.0017e+00,\n",
       "           -1.0617e+00, -9.7169e-01]],\n",
       "\n",
       "         [[-3.5683e-01, -4.1371e-01, -4.4215e-01,  ..., -1.3329e-03,\n",
       "            2.7107e-02,  1.2887e-02],\n",
       "          [-3.7105e-01, -4.4215e-01, -4.4215e-01,  ..., -7.2433e-02,\n",
       "           -7.2433e-02, -1.1509e-01],\n",
       "          [-4.2793e-01, -4.4215e-01, -3.4261e-01,  ..., -7.2433e-02,\n",
       "           -7.2433e-02, -8.6653e-02],\n",
       "          ...,\n",
       "          [ 6.8123e-01,  7.6655e-01,  8.2343e-01,  ..., -9.9674e-01,\n",
       "           -8.9720e-01, -8.4032e-01],\n",
       "          [ 6.2435e-01,  7.0967e-01,  7.6655e-01,  ..., -1.0394e+00,\n",
       "           -1.0394e+00, -9.8252e-01],\n",
       "          [ 5.8169e-01,  6.6701e-01,  6.9545e-01,  ..., -1.1105e+00,\n",
       "           -1.1247e+00, -1.0536e+00]]]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  2533,  3309, 11122, 49407, 49407],\n",
       "        [49406,   320,  2533,   783,  3309, 11122, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[-9.3096e-01, -9.8935e-01, -1.0185e+00,  ...,  2.5152e-01,\n",
       "            3.5371e-01,  4.2670e-01],\n",
       "          [-9.4555e-01, -1.0185e+00, -1.0039e+00,  ...,  1.0553e-01,\n",
       "            1.4933e-01,  1.7853e-01],\n",
       "          [-1.0185e+00, -1.0039e+00, -8.8716e-01,  ...,  9.0935e-02,\n",
       "            9.0935e-02,  9.0935e-02],\n",
       "          ...,\n",
       "          [ 1.8573e+00,  1.8865e+00,  1.9011e+00,  ..., -4.2001e-01,\n",
       "           -3.0322e-01, -2.4483e-01],\n",
       "          [ 1.8427e+00,  1.8281e+00,  1.8427e+00,  ..., -5.0760e-01,\n",
       "           -5.0760e-01, -4.4921e-01],\n",
       "          [ 1.8135e+00,  1.7990e+00,  1.7844e+00,  ..., -5.8059e-01,\n",
       "           -6.2439e-01, -5.5140e-01]],\n",
       "\n",
       "         [[-1.7628e-01, -2.3631e-01, -2.6633e-01,  ...,  5.7411e-01,\n",
       "            6.3414e-01,  6.6415e-01],\n",
       "          [-1.9129e-01, -2.6633e-01, -2.8134e-01,  ...,  4.2403e-01,\n",
       "            4.3904e-01,  4.2403e-01],\n",
       "          [-2.5132e-01, -2.6633e-01, -1.9129e-01,  ...,  4.5404e-01,\n",
       "            4.2403e-01,  3.6400e-01],\n",
       "          ...,\n",
       "          [ 1.5646e+00,  1.5946e+00,  1.6247e+00,  ..., -8.6664e-01,\n",
       "           -7.6158e-01, -7.0155e-01],\n",
       "          [ 1.4896e+00,  1.5196e+00,  1.5646e+00,  ..., -9.4168e-01,\n",
       "           -9.2667e-01, -8.6664e-01],\n",
       "          [ 1.4446e+00,  1.4596e+00,  1.4896e+00,  ..., -1.0017e+00,\n",
       "           -1.0617e+00, -9.7169e-01]],\n",
       "\n",
       "         [[-3.5683e-01, -4.1371e-01, -4.4215e-01,  ..., -1.3329e-03,\n",
       "            2.7107e-02,  1.2887e-02],\n",
       "          [-3.7105e-01, -4.4215e-01, -4.4215e-01,  ..., -7.2433e-02,\n",
       "           -7.2433e-02, -1.1509e-01],\n",
       "          [-4.2793e-01, -4.4215e-01, -3.4261e-01,  ..., -7.2433e-02,\n",
       "           -7.2433e-02, -8.6653e-02],\n",
       "          ...,\n",
       "          [ 6.8123e-01,  7.6655e-01,  8.2343e-01,  ..., -9.9674e-01,\n",
       "           -8.9720e-01, -8.4032e-01],\n",
       "          [ 6.2435e-01,  7.0967e-01,  7.6655e-01,  ..., -1.0394e+00,\n",
       "           -1.0394e+00, -9.8252e-01],\n",
       "          [ 5.8169e-01,  6.6701e-01,  6.9545e-01,  ..., -1.1105e+00,\n",
       "           -1.1247e+00, -1.0536e+00]]]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model\n"
     ]
    }
   ],
   "source": [
    "from on_field_gear_compliance_hackathon.models import predict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(DATA_DIR, 'test/helmet_2.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a person wearing helmet', 'a person not wearing helmet']\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=720x1280 at 0x7F9C2C49C100>\n",
      "{'input_ids': tensor([[49406,   320,  2533,  3309, 11122, 49407, 49407],\n",
      "        [49406,   320,  2533,   783,  3309, 11122, 49407]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[[[-8.7256e-01, -9.8935e-01, -9.7475e-01,  ..., -5.5050e-02,\n",
      "            4.4130e-01,  4.2670e-01],\n",
      "          [-1.4857e+00, -1.5587e+00, -1.4127e+00,  ..., -6.9648e-02,\n",
      "            2.3692e-01,  3.3439e-03],\n",
      "          [-3.6162e-01, -3.9081e-01, -3.9081e-01,  ...,  1.0553e-01,\n",
      "            9.0935e-02,  1.0553e-01],\n",
      "          ...,\n",
      "          [ 5.2889e-01,  4.7049e-01,  4.2670e-01,  ...,  3.0991e-01,\n",
      "            2.5152e-01,  1.7853e-01],\n",
      "          [ 5.1429e-01,  5.2889e-01,  4.8509e-01,  ...,  3.2451e-01,\n",
      "            1.6393e-01,  2.3692e-01],\n",
      "          [ 4.7049e-01,  4.8509e-01,  4.7049e-01,  ...,  2.5152e-01,\n",
      "            2.3692e-01,  2.9531e-01]],\n",
      "\n",
      "         [[ 3.6400e-01,  2.7395e-01,  2.7395e-01,  ..., -5.6219e-02,\n",
      "            4.8406e-01,  4.6905e-01],\n",
      "          [ 3.8118e-03, -8.6235e-02,  6.3843e-02,  ..., -5.6219e-02,\n",
      "            2.7395e-01,  3.3827e-02],\n",
      "          [ 6.4915e-01,  6.1913e-01,  6.0412e-01,  ...,  1.3888e-01,\n",
      "            1.3888e-01,  1.6890e-01],\n",
      "          ...,\n",
      "          [ 5.2908e-01,  4.6905e-01,  4.2403e-01,  ...,  3.4899e-01,\n",
      "            2.8896e-01,  2.1392e-01],\n",
      "          [ 5.1408e-01,  5.2908e-01,  4.8406e-01,  ...,  3.6400e-01,\n",
      "            1.9891e-01,  2.7395e-01],\n",
      "          [ 4.6905e-01,  4.8406e-01,  4.6905e-01,  ...,  2.8896e-01,\n",
      "            2.7395e-01,  3.4899e-01]],\n",
      "\n",
      "         [[ 3.2573e-01,  2.4041e-01,  2.8307e-01,  ..., -1.4353e-01,\n",
      "            2.6885e-01,  2.6885e-01],\n",
      "          [-1.0087e-01, -1.8619e-01, -1.3329e-03,  ..., -1.5775e-01,\n",
      "            6.9767e-02, -1.4353e-01],\n",
      "          [ 6.8123e-01,  6.3857e-01,  6.8123e-01,  ...,  4.1327e-02,\n",
      "           -2.9773e-02, -1.3329e-03],\n",
      "          ...,\n",
      "          [ 5.3903e-01,  4.8215e-01,  4.3949e-01,  ...,  4.6793e-01,\n",
      "            4.1105e-01,  3.3995e-01],\n",
      "          [ 5.2481e-01,  5.3903e-01,  4.9637e-01,  ...,  4.8215e-01,\n",
      "            3.2573e-01,  4.1105e-01],\n",
      "          [ 4.8215e-01,  4.9637e-01,  4.8215e-01,  ...,  4.1105e-01,\n",
      "            3.8261e-01,  4.5371e-01]]]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a person not wearing helmet'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.helmet_or_not(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-moderation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
